{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script is EDA for scraping NYC zoning and regulation data from the planning.nyc.gov website. In the end, I'd like to create a research tool for NYC zoning and regulation and perhaps a search engine to help developers and policy makers find the appropriate piece of legistation.\n",
    "\n",
    "For now, we can create a structure table to collect all the data, but then we are going to have to create some type of strucutre to process the data in a meaningful way. For example, for one article there might be multiple chapters and in those chapters certain itmes might not be broken down but instead in one large text. This type of text data needs to be organize and filtered somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL link for NYC zoning\n",
    "url = 'https://zr.planning.nyc.gov'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Roman numeral list\n",
    "roman = ['i', 'ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'x', 'xi', 'xii', 'xiii', 'xiv', 'xv', 'xvi', 'xvii',\n",
    "        'xviii', 'xix', 'xx']\n",
    "\n",
    "# parameters\n",
    "article_parameter = '/article-'\n",
    "\n",
    "columns = ['State', 'City','Article_Number', 'Article_Link', 'Article_Text', 'Chapter_Number', 'Chapter_Link', 'Chapter_Text']\n",
    "\n",
    "df = pd.DataFrame(columns = columns)\n",
    "\n",
    "# BS\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "articleHTML = soup.findAll('article')\n",
    "\n",
    "# Loops to collect all the article links\n",
    "for i in range(len(articleHTML)):\n",
    "    one_article_tag = soup.findAll('a')[i]\n",
    "    link = one_article_tag['href']\n",
    "    # Make sure we are only collecting article related data\n",
    "    if(link.find(article_parameter) >= 0):\n",
    "        \n",
    "        # chapter parameter\n",
    "        chapter_parameter =  link + '/chapter-'\n",
    "        print(chapt)\n",
    "        # get article text\n",
    "        article_text = one_article_tag.find('div', attrs = {'class': 'field field--name-field-article-title field--type-string field--label-hidden field__item'})\n",
    "\n",
    "        #  find the chapters\n",
    "        url_article =  url + link\n",
    "        response_chapter = requests.get(url_article)\n",
    "        soup_chapter = BeautifulSoup(response_chapter.text, \"html.parser\")\n",
    "        chapterHTML = soup_chapter.find('div', attrs = {'class': 'article-chapters'}).find('div', attrs = {'class': 'view-content'}).findAll('div', attrs = {'class': 'views-row'})\n",
    "        \n",
    "        # traverse through chapters\n",
    "        for j in range(len(chapterHTML)):\n",
    "\n",
    "            one_chapter_tag = soup_chapter.findAll('a')[j]\n",
    "            link_chapter = one_chapter_tag['href']\n",
    "            if(link_chapter.find(chapter_parameter) >= 0):\n",
    "                print(link_chapter)\n",
    "            \n",
    "                # populate table with article and chapters\n",
    "                df = df.append({\n",
    "                'Article_Number': link,\n",
    "                'Article_Link': url_article,\n",
    "                'Article_Text': article_text.text,\n",
    "                'Chapter_Number': link_chapter,\n",
    "                'Chapter_Link': url_article + link_chapter\n",
    "            }, ignore_index = True)\n",
    "\n",
    "df['State'] = 'NY'\n",
    "df['City'] = 'NYC'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>City</th>\n",
       "      <th>Article_Number</th>\n",
       "      <th>Article_Link</th>\n",
       "      <th>Article_Text</th>\n",
       "      <th>Chapter_Number</th>\n",
       "      <th>Chapter_Link</th>\n",
       "      <th>Chapter_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [State, City, Article_Number, Article_Link, Article_Text, Chapter_Number, Chapter_Link, Chapter_Text]\n",
       "Index: []"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# workingon chapters\n",
    "url = 'https://zr.planning.nyc.gov/article-i'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "test = soup.find('div', attrs = {'class': 'article-chapters'}).find('div', attrs = {'class': 'view-content'}).findAll('div', attrs = {'class': 'views-row'})\n",
    "\n",
    "len(test)\n",
    "#test2 = test[1]\n",
    "#test2.findAll('a')[0]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"views-row\">\n",
       "<article about=\"/article-i/chapter-6\" class=\"node node--type-chapter node--view-mode-leftnav\" data-history-node-id=\"85\" role=\"article\">\n",
       "<a class=\"chapter-title\" href=\"/article-i/chapter-6\"><span class=\"field field--name-title field--type-string field--label-hidden\">Chapter 6</span>\n",
       " - Comprehensive Off-street Parking Regulations in the Long Island City Area</a>\n",
       "</article>\n",
       "</div>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link = '/article-i'\n",
    "url_article =  url + link\n",
    "response_chapter = requests.get(url_article)\n",
    "soup_chapter = BeautifulSoup(response_chapter.text, \"html.parser\")\n",
    "# need to fix the soup_chapter.find\n",
    "chapterHTML = soup_chapter.find('div', attrs = {'class': 'article-chapters'}).find('div', attrs = {'class': 'view-content'}).findAll('div', attrs = {'class': 'views-row'})\n",
    "\n",
    "chapterHTML[5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
